{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e4cc9aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T22:26:02.520670Z",
     "start_time": "2025-07-16T22:26:02.515604Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from google.genai.types import FunctionDeclaration, FunctionResponse\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b0da2d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T22:26:02.544725Z",
     "start_time": "2025-07-16T22:26:02.539779Z"
    }
   },
   "outputs": [],
   "source": [
    "known_weather_data = {\n",
    "    'berlin': 20.0\n",
    "}\n",
    "\n",
    "def get_weather(city: str) -> float:\n",
    "    city = city.strip().lower()\n",
    "\n",
    "    if city in known_weather_data:\n",
    "        return known_weather_data[city]\n",
    "\n",
    "    return round(random.uniform(-5, 35), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655199f1",
   "metadata": {},
   "source": [
    "### Q1. Define function description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dac1f658",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T22:26:02.602669Z",
     "start_time": "2025-07-16T22:26:02.597604Z"
    }
   },
   "outputs": [],
   "source": [
    "get_weather_tool = {\n",
    "    \"name\": \"get_weather\",\n",
    "    \"description\": \"Get the current weather for a given city.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"city\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The name of the city to get the weather for.\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"city\"]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "760f905b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T22:26:02.654477Z",
     "start_time": "2025-07-16T22:26:02.650452Z"
    }
   },
   "outputs": [],
   "source": [
    "tools: list[FunctionDeclaration] = [get_weather_tool]\n",
    "# tools = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd9ae2f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T22:26:02.698214Z",
     "start_time": "2025-07-16T22:26:02.691114Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def gemini_config(tools: list[FunctionDeclaration] = []) -> types.GenerateContentConfig:\n",
    "    if not tools:\n",
    "        return types.GenerateContentConfig()\n",
    "    gemini_tools = types.Tool(function_declarations=tools)\n",
    "    config = types.GenerateContentConfig(tools=[gemini_tools])\n",
    "    return config\n",
    "\n",
    "def llm(prompt: str, tools: list[FunctionDeclaration] = []) -> types.GenerateContentResponse:\n",
    "    gemini_key = os.getenv('GEMINI_API_KEY')\n",
    "    client = genai.Client(api_key=gemini_key)\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash-lite-preview-06-17\",\n",
    "        contents=prompt,\n",
    "        config=gemini_config(tools=tools),\n",
    "    )\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a6210c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T22:26:02.732408Z",
     "start_time": "2025-07-16T22:26:02.728916Z"
    }
   },
   "outputs": [],
   "source": [
    "question = \"What's the weather like in Berlin?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fe6c205",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T22:26:02.815746Z",
     "start_time": "2025-07-16T22:26:02.811911Z"
    }
   },
   "outputs": [],
   "source": [
    "# llm_response = llm(\n",
    "#     prompt=question,\n",
    "#     tools=tools\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbaeeb61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T22:26:02.877409Z",
     "start_time": "2025-07-16T22:26:02.874111Z"
    }
   },
   "outputs": [],
   "source": [
    "# llm_response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b48534a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T22:26:03.479988Z",
     "start_time": "2025-07-16T22:26:02.996547Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponse(\n",
       "  automatic_function_calling_history=[],\n",
       "  candidates=[\n",
       "    Candidate(\n",
       "      content=Content(\n",
       "        parts=[\n",
       "          Part(\n",
       "            function_call=FunctionCall(\n",
       "              args=<... Max depth ...>,\n",
       "              name=<... Max depth ...>\n",
       "            )\n",
       "          ),\n",
       "        ],\n",
       "        role='model'\n",
       "      ),\n",
       "      finish_reason=<FinishReason.STOP: 'STOP'>,\n",
       "      index=0\n",
       "    ),\n",
       "  ],\n",
       "  model_version='gemini-2.5-flash-lite-preview-06-17',\n",
       "  sdk_http_response=HttpResponse(\n",
       "    headers=<dict len=11>\n",
       "  ),\n",
       "  usage_metadata=GenerateContentResponseUsageMetadata(\n",
       "    candidates_token_count=15,\n",
       "    prompt_token_count=61,\n",
       "    prompt_tokens_details=[\n",
       "      ModalityTokenCount(\n",
       "        modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "        token_count=61\n",
       "      ),\n",
       "    ],\n",
       "    total_token_count=76\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# q1 = \"what's the weather like in Odesa?\"\n",
    "llm_response = llm(\n",
    "    prompt=question,\n",
    "    tools=tools\n",
    ")\n",
    "llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "164b3bdf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T22:26:03.521830Z",
     "start_time": "2025-07-16T22:26:03.513891Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FunctionCall(\n",
       "  args={\n",
       "    'city': 'Berlin'\n",
       "  },\n",
       "  name='get_weather'\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_response.candidates[0].content.parts[0].function_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8891eb54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T22:26:03.657128Z",
     "start_time": "2025-07-16T22:26:03.652679Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function execution result: 20.0\n"
     ]
    }
   ],
   "source": [
    "tool_call = llm_response.candidates[0].content.parts[0].function_call\n",
    "\n",
    "if tool_call.name == \"get_weather\":\n",
    "    result = get_weather(**tool_call.args)\n",
    "    print(f\"Function execution result: {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a09bdc12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T22:26:04.249520Z",
     "start_time": "2025-07-16T22:26:03.757288Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdk_http_response=HttpResponse(\n",
      "  headers=<dict len=11>\n",
      ") candidates=[Candidate(\n",
      "  content=Content(\n",
      "    parts=[\n",
      "      Part(\n",
      "        text='The weather in Berlin is 20 degrees.'\n",
      "      ),\n",
      "    ],\n",
      "    role='model'\n",
      "  ),\n",
      "  finish_reason=<FinishReason.STOP: 'STOP'>,\n",
      "  index=0\n",
      ")] create_time=None response_id=None model_version='gemini-2.5-flash-lite-preview-06-17' prompt_feedback=None usage_metadata=GenerateContentResponseUsageMetadata(\n",
      "  candidates_token_count=10,\n",
      "  prompt_token_count=92,\n",
      "  prompt_tokens_details=[\n",
      "    ModalityTokenCount(\n",
      "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
      "      token_count=92\n",
      "    ),\n",
      "  ],\n",
      "  total_token_count=102\n",
      ") automatic_function_calling_history=[] parsed=None\n",
      "The weather in Berlin is 20 degrees.\n"
     ]
    }
   ],
   "source": [
    "function_response_part = types.Part.from_function_response(\n",
    "    name=tool_call.name,\n",
    "    response={\"result\": result},\n",
    ")\n",
    "\n",
    "contents = [\n",
    "    # types.Content(role=\"user\", parts=[types.Part.from_text(question)]),\n",
    "    # types.Content(role=\"assistant\", parts=[llm_response.candidates[0].content]),\n",
    "    types.Content(role=\"user\", parts=[types.Part(text=question)])\n",
    "]\n",
    "# Append function call and result of the function execution to contents\n",
    "contents.append(llm_response.candidates[0].content) # Append the content from the model's response.\n",
    "contents.append(types.Content(role=\"user\", parts=[function_response_part])) # Append the function response\n",
    "\n",
    "final_response = llm(\n",
    "    prompt=contents,\n",
    "    tools=tools\n",
    ")\n",
    "\n",
    "print(final_response)\n",
    "print(final_response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c19217",
   "metadata": {},
   "source": [
    "### Q2. Adding another tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7d9a230",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T22:26:04.273591Z",
     "start_time": "2025-07-16T22:26:04.268169Z"
    }
   },
   "outputs": [],
   "source": [
    "### Q2. Adding another tool\n",
    "\n",
    "def set_weather(city: str, temp: float) -> None:\n",
    "    city = city.strip().lower()\n",
    "    known_weather_data[city] = temp\n",
    "    return 'OK'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64d98b93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T22:26:04.309365Z",
     "start_time": "2025-07-16T22:26:04.303902Z"
    }
   },
   "outputs": [],
   "source": [
    "set_weather_tool = {\n",
    "    \"name\": \"set_weather\",\n",
    "    \"description\": \"Set the weather for a given city.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"city\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The name of the city to set the weather for.\"\n",
    "            },\n",
    "            \"temp\": {\n",
    "                \"type\": \"number\",\n",
    "                \"description\": \"The temperature to set for the city.\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"city\", \"temp\"]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4a346dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T22:26:04.384216Z",
     "start_time": "2025-07-16T22:26:04.375510Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered tool: set_weather\n",
      "Description: {'name': 'set_weather', 'description': 'Set the weather for a given city.', 'parameters': {'type': 'object', 'properties': {'city': {'type': 'string', 'description': 'The name of the city to set the weather for.'}, 'temp': {'type': 'number', 'description': 'The temperature to set for the city.'}}, 'required': ['city', 'temp']}}\n",
      "Registered tool: get_weather\n",
      "Description: {'name': 'get_weather', 'description': 'Get the current weather for a given city.', 'parameters': {'type': 'object', 'properties': {'city': {'type': 'string', 'description': 'The name of the city to get the weather for.'}}, 'required': ['city']}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from chat_assistant_gemini import ToolManager, ChatUserInterface, AIConversationAssistant\n",
    "\n",
    "tool_manager = ToolManager()\n",
    "tool_manager.register_tool(\n",
    "    set_weather, \n",
    "    set_weather_tool\n",
    "    # FunctionDeclaration(\n",
    "    #         name=\"set_weather\",\n",
    "    #         description=\"Set the weather for a given city.\",\n",
    "    #         parameters={\n",
    "    #             \"type\": \"object\",\n",
    "    #             \"properties\": {\n",
    "    #                 \"city\": {\"type\": \"string\", \n",
    "    #                          \"description\": \"The name of the city to set the weather for, e.g. Odesa, UA\"\n",
    "    #                 },\n",
    "    #                 \"temp\": {\n",
    "    #                     \"type\": \"number\",\n",
    "    #                     \"description\": \"The temperature to set for the city.\"\n",
    "    #                 }\n",
    "    #             },\n",
    "    #             \"required\": [\"city\", \"temp\"]\n",
    "    #         },\n",
    "    # )\n",
    ")\n",
    "\n",
    "get_weather_tool = {\n",
    "        \"name\":\"get_weather\",\n",
    "        \"description\":\"Get the current weather for a given city.\",\n",
    "        \"parameters\":{\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"city\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The name of the city to get the weather for.\"\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"city\"]\n",
    "        }\n",
    "}\n",
    "\n",
    "tool_manager.register_tool(\n",
    "    get_weather,\n",
    "    get_weather_tool\n",
    "    # FunctionDeclaration(\n",
    "    #     name=\"get_weather\",\n",
    "    #     description=\"Get the current weather for a given city.\",\n",
    "    #     parameters={\n",
    "    #         \"type\": \"object\",\n",
    "    #         \"properties\": {\n",
    "    #             \"city\": {\n",
    "    #                 \"type\": \"string\",\n",
    "    #                 \"description\": \"The name of the city to get the weather for.\"\n",
    "    #             }\n",
    "    #         },\n",
    "    #         \"required\": [\"city\"]\n",
    "    #     },\n",
    "    # )\n",
    ")\n",
    "\n",
    "\n",
    "ui = ChatUserInterface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f644ee3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T22:26:04.600007Z",
     "start_time": "2025-07-16T22:26:04.591928Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'set_weather': <function __main__.set_weather(city: str, temp: float) -> None>,\n",
       " 'get_weather': <function __main__.get_weather(city: str) -> float>}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_manager.functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5323ca0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T22:26:04.753852Z",
     "start_time": "2025-07-16T22:26:04.747157Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'set_weather',\n",
       "  'description': 'Set the weather for a given city.',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'city': {'type': 'string',\n",
       "     'description': 'The name of the city to set the weather for.'},\n",
       "    'temp': {'type': 'number',\n",
       "     'description': 'The temperature to set for the city.'}},\n",
       "   'required': ['city', 'temp']}},\n",
       " {'name': 'get_weather',\n",
       "  'description': 'Get the current weather for a given city.',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'city': {'type': 'string',\n",
       "     'description': 'The name of the city to get the weather for.'}},\n",
       "   'required': ['city']}}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_manager.get_tool_descriptions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d48d5a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T22:26:05.119166Z",
     "start_time": "2025-07-16T22:26:04.881650Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'object'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/training/llm-camp/.venv/lib/python3.12/site-packages/proto/marshal/rules/message.py:36\u001b[39m, in \u001b[36mMessageRule.to_proto\u001b[39m\u001b[34m(self, value)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     35\u001b[39m     \u001b[38;5;66;03m# Try the fast path first.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_descriptor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m     38\u001b[39m     \u001b[38;5;66;03m# If we have a TypeError, ValueError or AttributeError,\u001b[39;00m\n\u001b[32m     39\u001b[39m     \u001b[38;5;66;03m# try the slow path in case the error\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     44\u001b[39m     \u001b[38;5;66;03m# - a missing key issue due to nested struct. See: https://github.com/googleapis/proto-plus-python/issues/424.\u001b[39;00m\n\u001b[32m     45\u001b[39m     \u001b[38;5;66;03m# - a missing key issue due to nested duration. See: https://github.com/googleapis/google-cloud-python/issues/13350.\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Protocol message Schema has no \"type\" field.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m developer_prompt = \u001b[33m\"\u001b[39m\u001b[33mYou are a helpful assistant that can answer questions and use tools. If the user asks about weather, use the `get_weather` tool. If the user asks about set weather, use the `set_weather` tool.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m assistant = \u001b[43mAIConversationAssistant\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtool_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtool_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdeveloper_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeveloper_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mui_interface\u001b[49m\u001b[43m=\u001b[49m\u001b[43mui\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgemini-2.5-flash-lite-preview-06-17\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m assistant.start_chat()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/training/llm-camp/task_0a/chat_assistant_gemini.py:165\u001b[39m, in \u001b[36mAIConversationAssistant.__init__\u001b[39m\u001b[34m(self, tool_manager, developer_prompt, ui_interface, api_key, model_name)\u001b[39m\n\u001b[32m    161\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mError: GEMINI_API_KEY environment variable not set. Please provide it or set the env var.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    162\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mGEMINI_API_KEY environment variable not set\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m \u001b[38;5;28mself\u001b[39m.model = \u001b[43mgenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGenerativeModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtool_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_tool_descriptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[38;5;66;03m# Initialize chat history. Gemini models use a `start_chat` method.\u001b[39;00m\n\u001b[32m    171\u001b[39m \u001b[38;5;66;03m# The history will be managed internally by the ChatSession object.\u001b[39;00m\n\u001b[32m    172\u001b[39m \u001b[38;5;28mself\u001b[39m.chat_session = \u001b[38;5;28mself\u001b[39m.model.start_chat(history=[\n\u001b[32m    173\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mparts\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[38;5;28mself\u001b[39m.developer_prompt]}, \u001b[38;5;66;03m# Developer prompt goes as a user message\u001b[39;00m\n\u001b[32m    174\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mparts\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mUnderstood. I\u001b[39m\u001b[33m'\u001b[39m\u001b[33mm ready to assist.\u001b[39m\u001b[33m\"\u001b[39m]} \u001b[38;5;66;03m# Model acknowledges\u001b[39;00m\n\u001b[32m    175\u001b[39m ])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/training/llm-camp/.venv/lib/python3.12/site-packages/google/generativeai/generative_models.py:87\u001b[39m, in \u001b[36mGenerativeModel.__init__\u001b[39m\u001b[34m(self, model_name, safety_settings, generation_config, tools, tool_config, system_instruction)\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;28mself\u001b[39m._safety_settings = safety_types.to_easy_safety_dict(safety_settings)\n\u001b[32m     86\u001b[39m \u001b[38;5;28mself\u001b[39m._generation_config = generation_types.to_generation_config_dict(generation_config)\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m \u001b[38;5;28mself\u001b[39m._tools = \u001b[43mcontent_types\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_function_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tool_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     90\u001b[39m     \u001b[38;5;28mself\u001b[39m._tool_config = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/training/llm-camp/.venv/lib/python3.12/site-packages/google/generativeai/types/content_types.py:917\u001b[39m, in \u001b[36mto_function_library\u001b[39m\u001b[34m(lib)\u001b[39m\n\u001b[32m    915\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m917\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFunctionLibrary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlib\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/training/llm-camp/.venv/lib/python3.12/site-packages/google/generativeai/types/content_types.py:855\u001b[39m, in \u001b[36mFunctionLibrary.__init__\u001b[39m\u001b[34m(self, tools)\u001b[39m\n\u001b[32m    854\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, tools: Iterable[ToolType]):\n\u001b[32m--> \u001b[39m\u001b[32m855\u001b[39m     tools = \u001b[43m_make_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    856\u001b[39m     \u001b[38;5;28mself\u001b[39m._tools = \u001b[38;5;28mlist\u001b[39m(tools)\n\u001b[32m    857\u001b[39m     \u001b[38;5;28mself\u001b[39m._index = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/training/llm-camp/.venv/lib/python3.12/site-packages/google/generativeai/types/content_types.py:898\u001b[39m, in \u001b[36m_make_tools\u001b[39m\u001b[34m(tools)\u001b[39m\n\u001b[32m    896\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mThe only string that can be passed as a tool is \u001b[39m\u001b[33m'\u001b[39m\u001b[33mcode_execution\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    897\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tools, Iterable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tools, Mapping):\n\u001b[32m--> \u001b[39m\u001b[32m898\u001b[39m     tools = [\u001b[43m_make_tool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tools]\n\u001b[32m    899\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tools) > \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(t.function_declarations) == \u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tools):\n\u001b[32m    900\u001b[39m         \u001b[38;5;66;03m# flatten into a single tool.\u001b[39;00m\n\u001b[32m    901\u001b[39m         tools = [_make_tool([t.function_declarations[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tools])]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/training/llm-camp/.venv/lib/python3.12/site-packages/google/generativeai/types/content_types.py:823\u001b[39m, in \u001b[36m_make_tool\u001b[39m\u001b[34m(tool)\u001b[39m\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    822\u001b[39m         fd = tool\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m Tool(function_declarations=[\u001b[43mprotos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFunctionDeclaration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfd\u001b[49m\u001b[43m)\u001b[49m])\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tool, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    825\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tool.lower() == \u001b[33m\"\u001b[39m\u001b[33mcode_execution\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/training/llm-camp/.venv/lib/python3.12/site-packages/proto/message.py:728\u001b[39m, in \u001b[36mMessage.__init__\u001b[39m\u001b[34m(self, mapping, ignore_unknown_fields, **kwargs)\u001b[39m\n\u001b[32m    722\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    724\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    725\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnknown field for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, key)\n\u001b[32m    726\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m728\u001b[39m pb_value = \u001b[43mmarshal\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_proto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpb_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pb_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     params[key] = pb_value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/training/llm-camp/.venv/lib/python3.12/site-packages/proto/marshal/marshal.py:235\u001b[39m, in \u001b[36mBaseMarshal.to_proto\u001b[39m\u001b[34m(self, proto_type, value, strict)\u001b[39m\n\u001b[32m    232\u001b[39m     recursive_type = \u001b[38;5;28mtype\u001b[39m(proto_type().value)\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {k: \u001b[38;5;28mself\u001b[39m.to_proto(recursive_type, v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m value.items()}\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m pb_value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_rule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproto_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproto_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_proto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[38;5;66;03m# Sanity check: If we are in strict mode, did we get the value we want?\u001b[39;00m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m strict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pb_value, proto_type):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/training/llm-camp/.venv/lib/python3.12/site-packages/proto/marshal/rules/message.py:46\u001b[39m, in \u001b[36mMessageRule.to_proto\u001b[39m\u001b[34m(self, value)\u001b[39m\n\u001b[32m     36\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._descriptor(**value)\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m     38\u001b[39m         \u001b[38;5;66;03m# If we have a TypeError, ValueError or AttributeError,\u001b[39;00m\n\u001b[32m     39\u001b[39m         \u001b[38;5;66;03m# try the slow path in case the error\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     44\u001b[39m         \u001b[38;5;66;03m# - a missing key issue due to nested struct. See: https://github.com/googleapis/proto-plus-python/issues/424.\u001b[39;00m\n\u001b[32m     45\u001b[39m         \u001b[38;5;66;03m# - a missing key issue due to nested duration. See: https://github.com/googleapis/google-cloud-python/issues/13350.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m._pb\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/training/llm-camp/.venv/lib/python3.12/site-packages/proto/message.py:728\u001b[39m, in \u001b[36mMessage.__init__\u001b[39m\u001b[34m(self, mapping, ignore_unknown_fields, **kwargs)\u001b[39m\n\u001b[32m    722\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    724\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    725\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnknown field for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, key)\n\u001b[32m    726\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m728\u001b[39m pb_value = \u001b[43mmarshal\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_proto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpb_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pb_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     params[key] = pb_value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/training/llm-camp/.venv/lib/python3.12/site-packages/proto/marshal/marshal.py:235\u001b[39m, in \u001b[36mBaseMarshal.to_proto\u001b[39m\u001b[34m(self, proto_type, value, strict)\u001b[39m\n\u001b[32m    232\u001b[39m     recursive_type = \u001b[38;5;28mtype\u001b[39m(proto_type().value)\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {k: \u001b[38;5;28mself\u001b[39m.to_proto(recursive_type, v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m value.items()}\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m pb_value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_rule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproto_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproto_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_proto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[38;5;66;03m# Sanity check: If we are in strict mode, did we get the value we want?\u001b[39;00m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m strict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pb_value, proto_type):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/training/llm-camp/.venv/lib/python3.12/site-packages/proto/marshal/rules/enums.py:56\u001b[39m, in \u001b[36mEnumRule.to_proto\u001b[39m\u001b[34m(self, value)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# If a string is provided that matches an enum value, coerce it\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# to the enum value.\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_enum\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m]\u001b[49m.value\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# We got a pure integer; pass it on.\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/Cellar/python@3.12/3.12.10/Frameworks/Python.framework/Versions/3.12/lib/python3.12/enum.py:813\u001b[39m, in \u001b[36mEnumType.__getitem__\u001b[39m\u001b[34m(cls, name)\u001b[39m\n\u001b[32m    809\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, name):\n\u001b[32m    810\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    811\u001b[39m \u001b[33;03m    Return the member matching `name`.\u001b[39;00m\n\u001b[32m    812\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m813\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_member_map_\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[31mKeyError\u001b[39m: 'object'"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"You are a helpful assistant that can answer questions and use tools. If the user asks about weather, use the `get_weather` tool. If the user asks about set weather, use the `set_weather` tool.\"\n",
    "\n",
    "assistant = AIConversationAssistant(\n",
    "    tool_manager=tool_manager,\n",
    "    developer_prompt=developer_prompt,\n",
    "    ui_interface=ui,\n",
    "    model_name='gemini-2.5-flash-lite-preview-06-17'\n",
    ")\n",
    "\n",
    "\n",
    "assistant.start_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd2e8e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
